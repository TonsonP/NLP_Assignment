{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry potter story generator using GPT-2 (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchdata\n",
    "import torchtext\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "# SEED = 555\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I used the data from https://github.com/ErikaJacobs/Harry-Potter-Text-Mining\n",
    "import os \n",
    "path = './data' # assign directory\n",
    "hp_files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "hp_files = hp_files[0:2] # Use only two files for testing.\n",
    "len(hp_files) #7 files, each file indicate each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Text     36 non-null     object\n",
      " 1   Chapter  36 non-null     int64 \n",
      " 2   Book     36 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "                                                Text  Chapter  Book\n",
      "0  THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...        1     1\n",
      "1  THE VANISHING GLASS  Nearly ten years had pass...        2     1\n",
      "2  THE LETTERS FROM NO ONE  The escape of the Bra...        3     1\n",
      "3  THE KEEPER OF THE KEYS  BOOM. They knocked aga...        4     1\n",
      "4  DIAGON ALLEY  Harry woke early the next mornin...        5     1\n"
     ]
    }
   ],
   "source": [
    "# Load the data into dataframes\n",
    "import pandas as pd\n",
    "# Modified from the code provided by \n",
    "Book_list = list()\n",
    "\n",
    "df = pd.read_csv(hp_files[0], sep=\"@\")\n",
    "df.head()\n",
    "\n",
    "flag_token = 0\n",
    "for book in hp_files:\n",
    "    if flag_token == 0:\n",
    "        df = pd.read_csv(book, sep=\"@\")\n",
    "        flag_token = 1\n",
    "    else:\n",
    "        df2 = pd.read_csv(book, sep=\"@\")\n",
    "        df = pd.concat([df, df2])\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperated into smaller sentences\n",
    "temp_list = list()\n",
    "for idx, sentence in df.iterrows():\n",
    "    temp = sentence.Text.strip().lower().split()\n",
    "    chunks = [\" \".join(temp[x:x+30]) for x in range(0, len(temp), 30)]\n",
    "    temp_list.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 15:55:27.855395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 15:55:30.962345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-25 15:55:30.962687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-25 15:55:30.962698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-25 15:55:32.482744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-25 15:55:32.483134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-25 15:55:32.483165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# Clean dataset a bit.\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \n",
    "    # Remove backslash\n",
    "    sentence.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Clear the html tag by using regular expression.\n",
    "    sentence = re.sub(\"<[^>]*>\", \"\", sentence) # Filter html tag\n",
    "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) # Filter non-English\n",
    "    sentence = re.sub(\"/[^a-zA-Z0-9 ]/\", \"\", sentence) # Filter out some symbol\n",
    "    #It matches any character which is not contained in the ASCII character set (0-127, i.e. 0x0 to 0x7F)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # This time \"I WILL NOT FILTERS OUT STOPWORD\" during the preprocessing as I think it is\n",
    "    # necessary for story. For the punctuation I think it should be fine to filter out.\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
    "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# #let's apply to the whole dataframe\n",
    "# for i, row in df.iterrows():\n",
    "#     clean_text = preprocessing(row.Text)\n",
    "#     df.at[i, 'Clean_text'] = clean_text\n",
    "\n",
    "data = list()\n",
    "for sentence in temp_list:\n",
    "     sentence_clean = preprocessing(sentence)\n",
    "     data.append(sentence_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds_train, ds_valid = train_test_split(data, test_size=0.3, random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content'],\n",
       "        num_rows: 3862\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['content'],\n",
       "        num_rows: 1656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "raw_datasets_train = Dataset.from_pandas(pd.DataFrame(data = {'content': ds_train}))\n",
    "raw_datasets_valid = Dataset.from_pandas(pd.DataFrame(data = {'content': ds_valid}))\n",
    "\n",
    "#remove .shuffle if you want to train the whole dataset....\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        'train':raw_datasets_train,\n",
    "        'valid':raw_datasets_valid\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENT: bit with a mum an dad like your what else would yeh be an i reckon it be abou time yeh read yer letter.\\ harry stretch out his hand at last\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 2\n",
      "tensor([[ 2545,   351,   257, 25682,   281,  9955,   588,   534,   644,  2073,\n",
      "           561,  9838,    71,   307,   281,  1312, 43851,   340,   307,   450,\n",
      "           280,   640,  9838,    71,  1100,   331,   263,  3850,    13,    59,\n",
      "          3971,   563,  7539,   503,   465,  1021,   379,   938],\n",
      "        [ 1462,   883,   508,  1265,   329,   340,    13,    59,   475,   644,\n",
      "           922,   307,   777,  1573,   508,  3446,   307,   484, 11691,   284,\n",
      "          1265,   329,  1037,   618,  2506,   307,   655,   355, 10416,   290,\n",
      "         12008,   355, 50256, 50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"right\" # \"left\" or \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"], \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(outputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de59d5518c70413391c49097caca1ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cb9fa747bc4b87aa107f0dcbdb991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3862\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"], \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    input_batch = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "# raw_datasets_ = Dataset.from_pandas(pd.DataFrame(data=raw_datasets_train))\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword has not single token: harry\n",
      "Keyword has not single token: potter\n",
      "Keyword has not single token: voldemort\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"harry\",\n",
    "    \"potter\",\n",
    "    \"voldemort\"\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False) #change to reduction=None\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    # weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "    #     axis=[0, 2]\n",
    "    # )\n",
    "    # weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    # weighted_loss = (loss_per_sample * weights).mean()\n",
    "    weighted_loss = loss_per_sample.mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokenized_datasets['valid']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets['valid']['input_ids'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = list()\n",
    "# for i in range(1656):\n",
    "#     if tokenized_datasets['train']['input_ids'][i].shape[0] != 62:\n",
    "#         print(f\"lenght = {tokenized_datasets['train']['input_ids'][i].shape[0]}\")\n",
    "#         print(f'index = {i}')\n",
    "#         temp.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "#train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True)\n",
    "\n",
    "# Reduce the size otherwise, my pc will melted.\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train']['input_ids'][0:800], batch_size=16, shuffle=True)\n",
    "eval_dataloader  = DataLoader(tokenized_datasets[\"train\"]['input_ids'][800:1000], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(batch[\"input_ids\"].to(device), labels=batch[\"input_ids\"].to(device))\n",
    "            outputs.loss = outputs.loss.reshape(1)\n",
    "        losses.append(accelerator.gather(outputs.loss))        \n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (!f()).\n",
      "Your token has been saved to /root/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!f() { /root/.vscode-server/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/node /tmp/vscode-remote-containers-f9eebabf-4ff7-4b68-a8c5-6466c52d9040.js git-credential-helper $*; }; f\n"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TonsonP/Harry_potter_story_generator'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"Harry_potter_story_generator\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/NLP/Assignment/Huggingface_Story_Generator/Harry_potter_story_generator is already a clone of https://huggingface.co/TonsonP/Harry_potter_story_generator. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "output_dir = \"Harry_potter_story_generator\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #outputs = model(batch[\"input_ids\"].to(device), labels=batch[\"input_ids\"].to(device))\n",
    "            outputs = model(batch.to(device), labels=batch.to(device))\n",
    "            outputs.loss = outputs.loss.reshape(1)\n",
    "        losses.append(accelerator.gather(outputs.loss))        \n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.572465896606445, 39044.8359375)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d52a3a516f4ac8b92c17d07a8ddbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3225, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3197, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3228, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3284, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3192, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3248, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3214, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3194, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3312, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3180, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3159, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3246, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3265, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3201, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3284, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3222, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3031, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3075, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3042, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3132, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.3030, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2775, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2762, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2738, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2714, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2782, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2709, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2718, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2364, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2256, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2271, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2272, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2334, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.2285, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1723, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1790, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1790, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1793, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.1095, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5_000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        #logits = model(batch[\"input_ids\"]).logits\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch).logits\n",
    "        #loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        loss = keytoken_weighted_loss(batch, logits, keytoken_ids)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        print(loss)\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "        #if step % 1000 == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(repo_name, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(repo_name)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('https://huggingface.co/TonsonP/Harry_potter_story_generator/commit/7efe5be85690ac6e5b1b7b63a43016e4c55d076e',\n",
       " [push command, status code: running, in progress. PID: 5681])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Commit after finish training\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "repo.push_to_hub(commit_message=f\"Training in progress step {step}\", blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not know why but it not save json so I need to save manually.\n",
    "model.config.to_json_file(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('TonsonP/Harry_potter_story_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"right\" # \"left\" or \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", max_length=100, pad_token_id=0, eos_token_id=0, model='TonsonP/Harry_potter_story_generator', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.Potter Drake conservation\n"
     ]
    }
   ],
   "source": [
    "# Not so good, but okay.\n",
    "txt = \"Mr.Potter\"\n",
    "print(pipe(txt, num_return_sequences=50)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa76c3f280547938b0438f2edc9722f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/765 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3045306ca39e4872b07f25017e2e5694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c756a5b6dd174548a23aff5ce684ba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e074077a627a4cee9bc0a4a3342ff1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff075379d5e0490886612db1f1f463f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a69a22919d4754b572eafded5a84ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare with the one available on the internet\n",
    "pipe2 = pipeline(\"text-generation\", max_length=100, pad_token_id=0, eos_token_id=0, model='ceostroff/harry-potter-gpt2-fanfiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.Potter â€“ the House-Elf is asking permission to use that elf's name as a way to defend himself. The other side, or the house elves, are being punished by this House as if you were the one who betrayed them. And if I say that House-Elves are being punished for any crime, I must state that no one is ever to make a mistake.\"It was that last phrase that Harry recognized from his father's book, and it was clear that he was\n"
     ]
    }
   ],
   "source": [
    "# Way better\n",
    "txt = \"\"\"Mr.Potter\"\"\"\n",
    "print(pipe2(txt, num_return_sequences=50)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a causal language model from scratch (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
