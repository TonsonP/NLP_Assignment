{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry Potter story generator using transformers based model.\n",
    "    In this project, we will create the transformers based model for story generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchdata\n",
    "import torchtext\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "# SEED = 555\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I used the data from https://github.com/ErikaJacobs/Harry-Potter-Text-Mining\n",
    "import os \n",
    "path = './data' # assign directory\n",
    "hp_files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "len(hp_files) #7 files, each file indicate each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200 entries, 0 to 36\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Text     200 non-null    object\n",
      " 1   Chapter  200 non-null    int64 \n",
      " 2   Book     200 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 6.2+ KB\n",
      "None\n",
      "                                                Text  Chapter  Book\n",
      "0  THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...        1     1\n",
      "1  THE VANISHING GLASS  Nearly ten years had pass...        2     1\n",
      "2  THE LETTERS FROM NO ONE  The escape of the Bra...        3     1\n",
      "3  THE KEEPER OF THE KEYS  BOOM. They knocked aga...        4     1\n",
      "4  DIAGON ALLEY  Harry woke early the next mornin...        5     1\n"
     ]
    }
   ],
   "source": [
    "# Load the data into dataframes\n",
    "import pandas as pd\n",
    "# Modified from the code provided by \n",
    "Book_list = list()\n",
    "\n",
    "df = pd.read_csv(hp_files[0], sep=\"@\")\n",
    "df.head()\n",
    "\n",
    "flag_token = 0\n",
    "for book in hp_files:\n",
    "    if flag_token == 0:\n",
    "        df = pd.read_csv(book, sep=\"@\")\n",
    "        flag_token = 1\n",
    "    else:\n",
    "        df2 = pd.read_csv(book, sep=\"@\")\n",
    "        df = pd.concat([df, df2])\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 15:26:12.802269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 15:26:13.951622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-12 15:26:13.951824: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-12 15:26:13.951836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-12 15:26:14.779284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 15:26:14.779609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 15:26:14.779636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# Clean dataset a bit.\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \n",
    "    # Clear the html tag by using regular expression.\n",
    "    sentence = re.sub(\"<[^>]*>\", \"\", sentence) # Filter html tag\n",
    "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) # Filter non-English\n",
    "    sentence = re.sub(\"/[^a-zA-Z0-9 ]/\", \"\", sentence) # Filter out some symbol\n",
    "    #It matches any character which is not contained in the ASCII character set (0-127, i.e. 0x0 to 0x7F)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # This time \"I WILL NOT FILTERS OUT STOPWORD\" during the preprocessing as I think it is\n",
    "    # necessary for story. For the punctuation I think it should be fine to filter out.\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
    "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "#let's apply to the whole dataframe\n",
    "for i, row in df.iterrows():\n",
    "    clean_text = preprocessing(row.Text)\n",
    "    df.at[i, 'Clean_text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 60 28\n",
      "<class 'list'> <class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train and test set\n",
    "train_set, test_set = train_test_split(df['Clean_text'], test_size=0.3, random_state=999)\n",
    "\n",
    "# Split train_set into train_set and validation_set \n",
    "train_set, val_set  = train_test_split(train_set, test_size=0.2, random_state=999)\n",
    "\n",
    "# Convert pandas series to list\n",
    "train_set = train_set.astype(str).values.tolist()\n",
    "test_set = test_set.astype(str).values.tolist()\n",
    "val_set = val_set.astype(str).values.tolist()\n",
    "\n",
    "print(len(train_set), len(test_set), len(val_set))\n",
    "print(type(train_set), type(test_set), type(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer for each dataset\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "\n",
    "tokenized_dataset_train = yield_tokens(train_set)\n",
    "tokenized_dataset_val = yield_tokens(val_set)\n",
    "tokenized_dataset_test = yield_tokens(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 6644\n",
      "['<unk>', '<pad>', '<sos>', '<eos>', 'the', 'be', 'and', 'he', 'to', 'of']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(preprocessing(text))\n",
    "\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_set), \n",
    "                                  min_freq=5,\n",
    "                                  specials=special_symbols,\n",
    "                                  special_first=True)\n",
    "        \n",
    "vocab_transform.set_default_index(UNK_IDX)   \n",
    "print('Vocab Size',len(vocab_transform))                         \n",
    "print(vocab_transform.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:       \n",
    "        #appends eos so we know it ends....so model learn how to end...                             \n",
    "        #tokens = example.append('<eos>') #end of sentence\n",
    "        example.append('<eos>')\n",
    "        #numericalize          \n",
    "        tokens = [vocab[token] for token in example] \n",
    "        data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab_transform['<pad>'] #++<----making sure our embedding layer ignores pad\n",
    "\n",
    "text_pipeline = lambda x: vocab_transform(tokenizer(x))\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    #criterion expects float labels\n",
    "    return pad_sequence(text_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/train_loader.pkl', 'wb')\n",
    "# pickle.dump(train_loader, file)\n",
    "\n",
    "# file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/valid_loader.pkl', 'wb')\n",
    "# pickle.dump(valid_loader, file)\n",
    "\n",
    "# file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/test_loader.pkl', 'wb')\n",
    "# pickle.dump(test_loader, file)\n",
    "\n",
    "# file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/vocab_transform.pkl', 'wb')\n",
    "# pickle.dump(vocab_transform, file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads #make sure it's divisible....\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim,hid_dim) \n",
    "        self.fc_k = nn.Linear(hid_dim,hid_dim) \n",
    "        self.fc_v = nn.Linear(hid_dim,hid_dim) \n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(q)\n",
    "        K = self.fc_k(k)\n",
    "        V = self.fc_v(v)\n",
    "        \n",
    "        #Q, K, V = [b, l, h]\n",
    "        #reshape them into head_dim\n",
    "        #reshape them to [b, n_headm, l, head_dim]\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        #Q, K, V = [b, m_head, l, head_dim]\n",
    "\n",
    "        #e = QK/sqrt(dk)\n",
    "        e =  torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\n",
    "        #e = [b, n_heads, ql, kl]\n",
    "        \n",
    "        # torch.Size([64, 8, 50, 50])\n",
    "        # torch.Size([64, 1, 1, 50, 256])\n",
    "\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        a = torch.softmax(e, dim=-1)\n",
    "        #a = [batch size, n_heads, ql, kl]\n",
    "                    \n",
    "        #eV\n",
    "        x = torch.matmul(self.dropout(a),V)\n",
    "        #x : [b, n_heads, ql, head_di]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x: [b, ql, n_heads, head_dim]\n",
    "\n",
    "        #concat them together\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [b, ql, h]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        #x = [b, ql, h]\n",
    "\n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"./figures/GPT-1.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_ff = nn.LayerNorm(hid_dim) #second yellow box\n",
    "        self.norm_maskedatt = nn.LayerNorm(hid_dim) #first red box\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.ff = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, trg_mask):\n",
    "        #trg      : [b, l, h]\n",
    "        #enc_src  : [b, sl, h]\n",
    "        #trg_mask : [b, 1, tl, tl]\n",
    "        #src_mask : [b, 1, 1, sl]\n",
    "\n",
    "        #1st box : mask multi, add & norm\n",
    "        _trg, attention = self.self_attention(trg, trg, trg, trg_mask) #Q, K, V\n",
    "        _trg    = self.dropout(_trg)\n",
    "        _trg    = trg + _trg\n",
    "        trg     = self.norm_maskedatt(_trg)\n",
    "\n",
    "        #2rd box : ff, add & norm\n",
    "        _trg    = self.ff(trg)\n",
    "        _trg    = self.dropout(_trg)\n",
    "        _trg    = trg + _trg\n",
    "        trg     = self.norm_ff(_trg)\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads,\n",
    "                 pf_dim, dropout, device, src_pad_idx,trg_pad_idx, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_length, hid_dim)\n",
    "        self.trg_emb = nn.Embedding(output_dim, hid_dim)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "                            [\n",
    "                            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                            for _ in range(n_layers)\n",
    "                            ]\n",
    "                            )\n",
    "        self.fc = nn.Linear(hid_dim, output_dim)\n",
    "        self.device = device\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        #src = [batch size, src len]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_mask : [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg_mask.shape[-1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device =self.device)).bool() #lower triangle\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "        trg_mask = trg_mask & trg_sub_mask \n",
    "        #trg_mask : [batch size, 1, trg len, trg len]\n",
    "        return trg_mask     \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #src : = [batch size, trg len]\n",
    "        #enc_src : hidden state from encoder = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        trg_len = x.shape[1]\n",
    "        \n",
    "        src_mask = self.make_trg_mask(x)\n",
    "\n",
    "        #pos\n",
    "        pos = torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos = [batch size, trg len]\n",
    "\n",
    "        pos_emb = self.pos_emb(pos) #[batch size, trg len, hid dim]\n",
    "        trg_emb = self.trg_emb(x) #[batch size, trg len, hid dim]\n",
    "\n",
    "        x = pos_emb + trg_emb * self.scale #[batch size, trg len, hid dim]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers: #output, hidden\n",
    "            trg, attention = layer(x, src_mask)\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc(trg)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inference():\n",
    "    def __init__(self,decoder):\n",
    "        self.decoder = decoder\n",
    "\n",
    "    #use during inference\n",
    "    #encapsulates beam_decode or greedy_decode\n",
    "    def decode(self, src, src_len, trg, hidden, method='beam-search'):\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #src len = [batch size]\n",
    "\n",
    "        # encoder_outputs, hidden = self.encoder(src, src_len) \n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]  (*2 because of bidirectional)(every hidden states)\n",
    "        #hidden = [batch size, hid dim]  #final hidden state\n",
    "       \n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        if method == 'beam-search':\n",
    "            return self.beam_decode(src, trg, hidden)\n",
    "        else:\n",
    "            return self.greedy_decode(trg, hidden)\n",
    "\n",
    "    def greedy_decode(self, trg, decoder_hidden, encoder_outputs=None):\n",
    "        #trg = [batch size, src len]\n",
    "        #decoder_hiddens = [1, batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "\n",
    "        seq_len, batch_size = trg.size()\n",
    "        decoded_batch = torch.zeros((batch_size, seq_len))\n",
    "        # decoder_input = torch.LongTensor([[EN.vocab.stoi['<sos>']] for _ in range(batch_size)]).cuda()\n",
    "        decoder_input = trg[0, :] # sos\n",
    "        # print(decoder_input.shape)\n",
    "\n",
    "        # mask = self.create_mask(trg[:, idx].unsqueeze(1))\n",
    "\n",
    "        for t in range(seq_len): \n",
    "            prediction, decoder_hidden = self.decoder(decoder_input)\n",
    "            topv, topi = prediction.data.topk(1)  # [32, 10004] get candidates\n",
    "            topi = topi.view(-1)\n",
    "            decoded_batch[:, t] = topi\n",
    "\n",
    "            decoder_input = topi.detach().view(-1)\n",
    "\n",
    "        return decoded_batch #(batch size, length)\n",
    "\n",
    "    def beam_decode(self, src_tensor, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
    "        #src_tensor      = [src len, beam_decodebatch size]\n",
    "        #target_tensor   = [trg len, batch size]\n",
    "        #decoder_hiddens = [1, batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        #target_tensor = [batch size, trg len]\n",
    "        \n",
    "        #how many parallel searches\n",
    "        beam_width = 3\n",
    "        \n",
    "        #how many sentence do you want to generate\n",
    "        topk = 1  \n",
    "        \n",
    "        #final generated sentence\n",
    "        decoded_batch = []\n",
    "                \n",
    "        #Another difference is that beam_search_decoding has \n",
    "        #to be done sentence by sentence, thus the batch size is indexed and reduced to only 1.  \n",
    "        #To keep the dimension same, we unsqueeze 1 dimension for the batch size.\n",
    "        for idx in range(target_tensor.size(0)):  # batch_size\n",
    "            \n",
    "            #decoder_hiddens = [1, batch size, dec hid dim]\n",
    "            decoder_hidden = decoder_hiddens[:, idx, :]\n",
    "            #decoder_hidden = [1, dec hid dim]\n",
    "            \n",
    "            #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "            # encoder_output = encoder_outputs[:, idx, :].unsqueeze(1)\n",
    "            #encoder_output = [src len, 1, enc hid dim * 2]\n",
    "            \n",
    "            # mask = self.create_mask(src_tensor[:, idx].unsqueeze(1))\n",
    "            # print(\"mask shape: \", mask.shape)\n",
    "            \n",
    "            #mask = [1, src len]\n",
    "\n",
    "            # Start with the start of the sentence token\n",
    "            decoder_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "\n",
    "            # Number of sentence to generate\n",
    "            endnodes = []  #hold the nodes of EOS, so we can backtrack\n",
    "            number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "            # starting node -  hidden vector, previous node, word id, logp, length\n",
    "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "            nodes = PriorityQueue()  #this is a min-heap\n",
    "\n",
    "            # start the queue\n",
    "            nodes.put((-node.eval(), node))  #we need to put - because PriorityQueue is a min-heap\n",
    "            qsize = 1\n",
    "\n",
    "            # start beam search\n",
    "            while True:\n",
    "                # give up when decoding takes too long\n",
    "                if qsize > 2000: break\n",
    "\n",
    "                # fetch the best node\n",
    "                # score is log p divides by the length scaled by some constants\n",
    "                score, n = nodes.get()\n",
    "                            \n",
    "                # wordid is simply the numercalized integer of the word\n",
    "                decoder_input  = n.wordid\n",
    "                decoder_hidden = n.h\n",
    "\n",
    "                if n.wordid.item() == EOS_IDX and n.prevNode != None:\n",
    "                    endnodes.append((score, n))\n",
    "                    # if we reached maximum # of sentences required\n",
    "                    if len(endnodes) >= number_required:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # decode for one step using decoder\n",
    "                # decoder_input = SOS_IDX\n",
    "                # decoder_hidden = [1, hid dim]\n",
    "                # encoder_output = [src len, 1, hid dim * 2]\n",
    "                # mask = [1, src len]\n",
    "                \n",
    "                # prediction, decoder_hidden, _ = self.decoder(decoder_input)\n",
    "                #prediction     = [1, output dim]  #1 because the batch size is 1\n",
    "                #decoder hidden = [1, hid dim]\n",
    "\n",
    "                #so basically prediction is probabilities across all possible vocab\n",
    "                #we gonna retrieve k top probabilities (which is defined by beam_width) and their indexes\n",
    "                #recall that beam_width defines how many parallel searches we want\n",
    "                log_prob, indexes = torch.topk(prediction, beam_width)\n",
    "                # log_prob      = (1, beam width)\n",
    "                # indexes       = (1, beam width)\n",
    "                \n",
    "                nextnodes = []  #the next possible node you can move to\n",
    "\n",
    "                # we only select beam_width amount of nextnodes\n",
    "                for top in range(beam_width):\n",
    "                    pred_t = indexes[0, top].reshape(-1)  #reshape because wordid is assume to be []; see when we define SOS\n",
    "                    log_p  = log_prob[0, top].item()\n",
    "                                    \n",
    "                    #decoder hidden, previous node, current node, prob, length\n",
    "                    node = BeamSearchNode(decoder_hidden, n, pred_t, n.logp + log_p, n.len + 1)\n",
    "                    score = -node.eval()\n",
    "                    nextnodes.append((score, node))\n",
    "\n",
    "                # put them into queue\n",
    "                for i in range(len(nextnodes)):\n",
    "                    score, nn = nextnodes[i]\n",
    "                    nodes.put((score, nn))\n",
    "                    # increase qsize\n",
    "                qsize += len(nextnodes) - 1\n",
    "\n",
    "            # Once everything is finished, choose nbest paths, back trace them\n",
    "            \n",
    "            ## in case it does not finish, we simply get couple of nodes with highest probability\n",
    "            if len(endnodes) == 0:\n",
    "                endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "            #look from the end and go back....\n",
    "            utterances = []\n",
    "            for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "                utterance = []\n",
    "                utterance.append(n.wordid)\n",
    "                # back trace by looking at the previous nodes.....\n",
    "                while n.prevNode != None:\n",
    "                    n = n.prevNode\n",
    "                    utterance.append(n.wordid)\n",
    "\n",
    "                utterance = utterance[::-1]  #reverse it....\n",
    "                utterances.append(utterance) #append to the list of sentences....\n",
    "\n",
    "            decoded_batch.append(utterances)\n",
    "\n",
    "        return decoded_batch  #(batch size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        self.h        = hiddenstate  #define the hidden state\n",
    "        self.prevNode = previousNode  #where does it come from\n",
    "        self.wordid   = wordId  #the numericalized integer of the word\n",
    "        self.logp     = logProb  #the log probability\n",
    "        self.len      = length  #the current length; first word starts at 1\n",
    "\n",
    "    def eval(self, alpha=0.7):\n",
    "        # the score will be simply the log probability penaltized by the length \n",
    "        # we add some small number to avoid division error\n",
    "        # read https://arxiv.org/abs/1808.10006 to understand how alpha is selected\n",
    "        return self.logp / float(self.len + 1e-6) ** (alpha)\n",
    "    \n",
    "    #this is the function for comparing between two beamsearchnodes, whether which one is better\n",
    "    #it is called when you called \"put\"\n",
    "    def __lt__(self, other):\n",
    "        return self.len < other.len\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.len > other.len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (pos_emb): Embedding(100, 256)\n",
       "  (trg_emb): Embedding(6644, 256)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=6644, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim  = len(vocab_transform)\n",
    "hid_dim = 256\n",
    "dec_layers = 12\n",
    "dec_heads = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "model = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              dec_dropout, \n",
    "              device,SRC_PAD_IDX,TRG_PAD_IDX).to(device)\n",
    "\n",
    "# model = DecoderTransformer(decode, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we can print the complexity by the number of parameters\n",
    "# def count_parameters(model):\n",
    "#     params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "#     for item in params:\n",
    "#         print(f'{item:>6}')\n",
    "#     print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,759,220 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #this data is from get_data()\n",
    "    #train_data.shape # [batch_size, number of batches....]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, seq_len, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are calling our implemented beam-search function <code>decode</code>.  To show it, we print some sample so we can take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, seq_len):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = loader.shape[-1]\n",
    "    loader = loader[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = loader.shape[-1]\n",
    "\n",
    "    # hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        # hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, _, trg = get_batch(loader, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        output, attentions = model(src)\n",
    "\n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output = output.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        trg = trg.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    decoded_batch_list = []\n",
    "    # hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            # hidden = model.detach_hidden(hidden)\n",
    "            src, src_len, trg = get_batch(data, seq_len, idx)\n",
    "            src, trg = src.to(device), trg.to(device) #src,trg = batch_size,seq len\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            #prediction, hidden = model(src, 0, target)\n",
    "            prediction, attention = model(src)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            trg = trg.reshape(-1)\n",
    " \n",
    "            #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "            # decoded_batch = model.decode(src, src_len, trg, attention, method='beam-search')\n",
    "            # #len(decoded_batch) = 64\n",
    "            # #len(decoded_batch[0]) = 1 = number of sentence generated, i.e., topk            \n",
    "            # decoded_batch_list.append(decoded_batch)\n",
    "\n",
    "            #remove the first output (SOS) and then reshape both output and trg so we can calculate loss\n",
    "            # output_dim = prediction.shape[-1]\n",
    "            # prediction = prediction[1:].view(-1, output_dim)\n",
    "            # trg = trg[1:].view(-1)\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(prediction, trg)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "        \n",
    "        #this is optional; you don't have to; printing first three samples of the first batch\n",
    "        # print(\"print samples from first decode batch\")\n",
    "        # for sentence_index in decoded_batch_list[0][:3]:\n",
    "        #     decode_text_arr = [vocab_transform.lookup_token(i) for i in sentence_index[0]]\n",
    "        #     decode_sentence = \" \".join(decode_text_arr[1:-1])  #no need sos and eos\n",
    "        #     print(\"pred target : {}\".format(decode_sentence))\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_length = len(train_data)\n",
    "# val_loader_length   = len(valid_data)\n",
    "# test_loader_length  = len(test_data)\n",
    "# train_loader_length,val_loader_length,test_loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    train_data = i\n",
    "    break\n",
    "\n",
    "for j in valid_loader:\n",
    "    valid_data = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 8s\n",
      "\tTrain Loss: 6.263 | Train PPL: 524.870\n",
      "\t Val. Loss: 5.187 |  Val. PPL: 178.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 6s\n",
      "\tTrain Loss: 5.687 | Train PPL: 295.098\n",
      "\t Val. Loss: 4.928 |  Val. PPL: 138.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 5s\n",
      "\tTrain Loss: 5.321 | Train PPL: 204.636\n",
      "\t Val. Loss: 4.535 |  Val. PPL:  93.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 6s\n",
      "\tTrain Loss: 5.317 | Train PPL: 203.813\n",
      "\t Val. Loss: 4.430 |  Val. PPL:  83.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 5s\n",
      "\tTrain Loss: 5.044 | Train PPL: 155.093\n",
      "\t Val. Loss: 4.114 |  Val. PPL:  61.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 6s\n",
      "\tTrain Loss: 5.040 | Train PPL: 154.437\n",
      "\t Val. Loss: 4.055 |  Val. PPL:  57.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 5s\n",
      "\tTrain Loss: 5.486 | Train PPL: 241.292\n",
      "\t Val. Loss: 4.383 |  Val. PPL:  80.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 5s\n",
      "\tTrain Loss: 5.050 | Train PPL: 156.038\n",
      "\t Val. Loss: 4.312 |  Val. PPL:  74.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 6s\n",
      "\tTrain Loss: 4.848 | Train PPL: 127.506\n",
      "\t Val. Loss: 4.362 |  Val. PPL:  78.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 4s\n",
      "\tTrain Loss: 4.929 | Train PPL: 138.249\n",
      "\t Val. Loss: 4.650 |  Val. PPL: 104.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 0m 5s\n",
      "\tTrain Loss: 4.550 | Train PPL:  94.676\n",
      "\t Val. Loss: 4.367 |  Val. PPL:  78.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 0m 5s\n",
      "\tTrain Loss: 4.434 | Train PPL:  84.282\n",
      "\t Val. Loss: 4.306 |  Val. PPL:  74.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 0m 7s\n",
      "\tTrain Loss: 4.901 | Train PPL: 134.399\n",
      "\t Val. Loss: 4.820 |  Val. PPL: 123.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 0m 6s\n",
      "\tTrain Loss: 4.995 | Train PPL: 147.705\n",
      "\t Val. Loss: 4.940 |  Val. PPL: 139.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 0m 5s\n",
      "\tTrain Loss: 4.551 | Train PPL:  94.686\n",
      "\t Val. Loss: 4.475 |  Val. PPL:  87.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 0m 7s\n",
      "\tTrain Loss: 4.957 | Train PPL: 142.099\n",
      "\t Val. Loss: 4.918 |  Val. PPL: 136.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time: 0m 5s\n",
      "\tTrain Loss: 4.632 | Train PPL: 102.706\n",
      "\t Val. Loss: 4.568 |  Val. PPL:  96.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time: 0m 5s\n",
      "\tTrain Loss: 4.833 | Train PPL: 125.601\n",
      "\t Val. Loss: 4.782 |  Val. PPL: 119.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time: 0m 7s\n",
      "\tTrain Loss: 4.868 | Train PPL: 130.009\n",
      "\t Val. Loss: 4.829 |  Val. PPL: 125.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time: 0m 6s\n",
      "\tTrain Loss: 4.740 | Train PPL: 114.383\n",
      "\t Val. Loss: 4.689 |  Val. PPL: 108.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Time: 0m 6s\n",
      "\tTrain Loss: 4.839 | Train PPL: 126.339\n",
      "\t Val. Loss: 4.795 |  Val. PPL: 120.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Time: 0m 6s\n",
      "\tTrain Loss: 4.670 | Train PPL: 106.706\n",
      "\t Val. Loss: 4.615 |  Val. PPL: 101.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Time: 0m 7s\n",
      "\tTrain Loss: 5.177 | Train PPL: 177.085\n",
      "\t Val. Loss: 5.147 |  Val. PPL: 171.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Time: 0m 6s\n",
      "\tTrain Loss: 4.955 | Train PPL: 141.947\n",
      "\t Val. Loss: 4.914 |  Val. PPL: 136.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Time: 0m 6s\n",
      "\tTrain Loss: 4.634 | Train PPL: 102.944\n",
      "\t Val. Loss: 4.574 |  Val. PPL:  96.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Time: 0m 6s\n",
      "\tTrain Loss: 5.424 | Train PPL: 226.693\n",
      "\t Val. Loss: 5.409 |  Val. PPL: 223.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Time: 0m 6s\n",
      "\tTrain Loss: 5.328 | Train PPL: 206.026\n",
      "\t Val. Loss: 5.308 |  Val. PPL: 201.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Time: 0m 5s\n",
      "\tTrain Loss: 4.480 | Train PPL:  88.221\n",
      "\t Val. Loss: 4.412 |  Val. PPL:  82.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Time: 0m 7s\n",
      "\tTrain Loss: 5.158 | Train PPL: 173.796\n",
      "\t Val. Loss: 5.133 |  Val. PPL: 169.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Time: 0m 5s\n",
      "\tTrain Loss: 4.830 | Train PPL: 125.228\n",
      "\t Val. Loss: 4.780 |  Val. PPL: 119.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Time: 0m 6s\n",
      "\tTrain Loss: 5.201 | Train PPL: 181.513\n",
      "\t Val. Loss: 5.178 |  Val. PPL: 177.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Time: 0m 5s\n",
      "\tTrain Loss: 4.910 | Train PPL: 135.640\n",
      "\t Val. Loss: 4.868 |  Val. PPL: 130.097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Time: 0m 5s\n",
      "\tTrain Loss: 4.758 | Train PPL: 116.512\n",
      "\t Val. Loss: 4.703 |  Val. PPL: 110.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Time: 0m 6s\n",
      "\tTrain Loss: 4.893 | Train PPL: 133.344\n",
      "\t Val. Loss: 4.845 |  Val. PPL: 127.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Time: 0m 7s\n",
      "\tTrain Loss: 5.239 | Train PPL: 188.568\n",
      "\t Val. Loss: 5.216 |  Val. PPL: 184.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Time: 0m 6s\n",
      "\tTrain Loss: 4.507 | Train PPL:  90.668\n",
      "\t Val. Loss: 4.443 |  Val. PPL:  85.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Time: 0m 6s\n",
      "\tTrain Loss: 4.539 | Train PPL:  93.566\n",
      "\t Val. Loss: 4.470 |  Val. PPL:  87.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Time: 0m 6s\n",
      "\tTrain Loss: 4.970 | Train PPL: 143.998\n",
      "\t Val. Loss: 4.922 |  Val. PPL: 137.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Time: 0m 6s\n",
      "\tTrain Loss: 4.961 | Train PPL: 142.665\n",
      "\t Val. Loss: 4.923 |  Val. PPL: 137.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Time: 0m 6s\n",
      "\tTrain Loss: 4.968 | Train PPL: 143.690\n",
      "\t Val. Loss: 4.925 |  Val. PPL: 137.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Time: 0m 7s\n",
      "\tTrain Loss: 4.898 | Train PPL: 133.992\n",
      "\t Val. Loss: 4.856 |  Val. PPL: 128.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Time: 0m 6s\n",
      "\tTrain Loss: 4.971 | Train PPL: 144.177\n",
      "\t Val. Loss: 4.927 |  Val. PPL: 137.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Time: 0m 6s\n",
      "\tTrain Loss: 4.832 | Train PPL: 125.436\n",
      "\t Val. Loss: 4.778 |  Val. PPL: 118.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Time: 0m 6s\n",
      "\tTrain Loss: 5.072 | Train PPL: 159.457\n",
      "\t Val. Loss: 5.032 |  Val. PPL: 153.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Time: 0m 6s\n",
      "\tTrain Loss: 5.106 | Train PPL: 164.962\n",
      "\t Val. Loss: 5.060 |  Val. PPL: 157.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Time: 0m 6s\n",
      "\tTrain Loss: 4.481 | Train PPL:  88.310\n",
      "\t Val. Loss: 4.402 |  Val. PPL:  81.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Time: 0m 7s\n",
      "\tTrain Loss: 4.974 | Train PPL: 144.654\n",
      "\t Val. Loss: 4.937 |  Val. PPL: 139.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Time: 0m 6s\n",
      "\tTrain Loss: 5.730 | Train PPL: 307.937\n",
      "\t Val. Loss: 5.731 |  Val. PPL: 308.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Time: 0m 7s\n",
      "\tTrain Loss: 5.142 | Train PPL: 171.079\n",
      "\t Val. Loss: 5.113 |  Val. PPL: 166.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Time: 0m 6s\n",
      "\tTrain Loss: 4.829 | Train PPL: 125.107\n",
      "\t Val. Loss: 4.785 |  Val. PPL: 119.760\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 50 #<----for the sake of brevity\n",
    "clip       = 0.25\n",
    "seq_len    = 50\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in train_loader:\n",
    "        train_data = i\n",
    "        break\n",
    "\n",
    "    for j in valid_loader:\n",
    "        valid_data = i\n",
    "        break\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_data, optimizer, criterion, clip, seq_len)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.147 | Test PPL: 171.923 |\n"
     ]
    }
   ],
   "source": [
    "for i in test_loader:\n",
    "    test_data = i\n",
    "    break\n",
    "\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    # hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src)\n",
    "\n",
    "            # print(prediction.shape)\n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (pos_emb): Embedding(100, 256)\n",
       "  (trg_emb): Embedding(6644, 256)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): DecoderLayer(\n",
       "      (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_maskedatt): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=6644, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim  = len(vocab_transform)\n",
    "hid_dim = 256\n",
    "dec_layers = 12\n",
    "dec_heads = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "model = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              dec_dropout, \n",
    "              device,SRC_PAD_IDX,TRG_PAD_IDX).to(device)\n",
    "\n",
    "# model = DecoderTransformer(decode, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "he lost the key which be not aware that he be a bit of the fact that he be a great deal with a bit of the one to moody say harry say moody who be a bit of the air harry say moody who be a little close to moody who have not\n",
      "\n",
      "0.7\n",
      "he lost the key the wall beside the train as just see muggle say lupin who have read what be look nervous.\\\"this as he be if you be not to be say harry say ron ask fred shout moody look around ravenclaw at a long and gaze to moody say moody say moody anyway\n",
      "\n",
      "0.75\n",
      "he lost the key the wall beside the train as just see muggle say lupin who have read cluster around the air harry and hagrid and cling to close to turn a small point his eye moody she be i think what be he have to harry back to find to see sirius 's\n",
      "\n",
      "0.8\n",
      "he lost the key the wall beside the train as just see muggle say lupin shout towards a cluster around the air harry and hagrid and cling to close to turn a small point his eye moody she be i think what i think it be quite as she have to see all know\n",
      "\n",
      "1.0\n",
      "he lost the key with three people avada curious shadow swim at just see muggle death eater and trelawney in the many vision what be confident that the sword transpire be confirm that be so loudly turn assistance door it be it up the upper ravenclaw at a number of phineas cake to moody\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'he lost the key'\n",
    "max_seq_len = 50\n",
    "seed = 0\n",
    "model.load_state_dict(torch.load('/root/projects/NLP/Assignment/Transformer_Generator/models/Decoder.pt'))\n",
    "            #superdiverse   more diverse\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
    "#sample from this distribution higher probability will get more change\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab_transform, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he lost the key with three people avada curious shadow swim at just see muggle death eater and trelawney in the many vision what be confident that the sword transpire be confirm that be so loudly turn assistance door it be it up the upper ravenclaw at a number of phineas cake to moody'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/vocab_transforms.pkl', 'wb')\n",
    "pickle.dump(vocab_transform, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('/root/projects/NLP/Assignment/Transformer_Generator/obj/vocab_transforms.pkl', 'rb')\n",
    "test = pickle.load(file)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'be',\n",
       " 'and',\n",
       " 'he',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'have',\n",
       " 'harry',\n",
       " 'it',\n",
       " 'his',\n",
       " 'in',\n",
       " 'say',\n",
       " 'not',\n",
       " 'i',\n",
       " 'that',\n",
       " 'they',\n",
       " 'you',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'as',\n",
       " 'at',\n",
       " 'she',\n",
       " 'hermione',\n",
       " 'with',\n",
       " 'on',\n",
       " 'ron',\n",
       " 'we',\n",
       " 'for',\n",
       " 'but',\n",
       " 'look',\n",
       " 'know',\n",
       " 'from',\n",
       " 'c',\n",
       " 'out',\n",
       " 'into',\n",
       " 'her',\n",
       " 'would',\n",
       " 'up',\n",
       " 'could',\n",
       " 'all',\n",
       " 'think',\n",
       " 'see',\n",
       " 'what',\n",
       " 'there',\n",
       " 'get',\n",
       " 'who',\n",
       " 'go',\n",
       " 'back',\n",
       " 'wand',\n",
       " 'then',\n",
       " 'so',\n",
       " 'dumbledore',\n",
       " 'this',\n",
       " 'their',\n",
       " 'if',\n",
       " 'will',\n",
       " 'like',\n",
       " 'by',\n",
       " 'one',\n",
       " 'take',\n",
       " 'eye',\n",
       " 'over',\n",
       " 'come',\n",
       " 'make',\n",
       " 'an',\n",
       " 'tell',\n",
       " 'can',\n",
       " 'now',\n",
       " 'about',\n",
       " 'still',\n",
       " 'no',\n",
       " 'or',\n",
       " 'feel',\n",
       " 'around',\n",
       " 'hand',\n",
       " 'down',\n",
       " 'when',\n",
       " \"'\",\n",
       " 'more',\n",
       " 'ask',\n",
       " 'my',\n",
       " 'face',\n",
       " 'where',\n",
       " 'just',\n",
       " 'seem',\n",
       " 'your',\n",
       " 'again',\n",
       " 'leave',\n",
       " 'which',\n",
       " 'want',\n",
       " 'how',\n",
       " 'voldemort',\n",
       " 'through',\n",
       " 've',\n",
       " 'time',\n",
       " 'voice',\n",
       " 'other',\n",
       " 'turn',\n",
       " 'door',\n",
       " 'death',\n",
       " 'off',\n",
       " '.',\n",
       " 'before',\n",
       " 'after',\n",
       " 'try',\n",
       " 'little',\n",
       " 'find',\n",
       " 'hear',\n",
       " 'room',\n",
       " 'head',\n",
       " 'only',\n",
       " 'open',\n",
       " 'than',\n",
       " 'away',\n",
       " 'give',\n",
       " 'place',\n",
       " 'stand',\n",
       " 'here',\n",
       " 'long',\n",
       " 'himself',\n",
       " 'dark',\n",
       " 'never',\n",
       " 'though',\n",
       " 'even',\n",
       " 'well',\n",
       " 'two',\n",
       " 'moment',\n",
       " 'right',\n",
       " 'weasley',\n",
       " 'close',\n",
       " 'its',\n",
       " 'way',\n",
       " 'light',\n",
       " 'wizard',\n",
       " 'once',\n",
       " 'keep',\n",
       " 'hold',\n",
       " 'sit',\n",
       " 'old',\n",
       " 'point',\n",
       " 'sure',\n",
       " 'eater',\n",
       " 'something',\n",
       " 'move',\n",
       " 'upon',\n",
       " 'inside',\n",
       " '\\\\\"i',\n",
       " 'behind',\n",
       " 'thing',\n",
       " 'need',\n",
       " 'toward',\n",
       " 'much',\n",
       " 'first',\n",
       " 'cloak',\n",
       " 'lupin',\n",
       " 'sword',\n",
       " 'fall',\n",
       " 'snape',\n",
       " 'any',\n",
       " 'good',\n",
       " 'last',\n",
       " 'too',\n",
       " 'own',\n",
       " 'must',\n",
       " 'house',\n",
       " 'might',\n",
       " 'some',\n",
       " 'nothing',\n",
       " 'lie',\n",
       " 'pull',\n",
       " 'because',\n",
       " 'why',\n",
       " 'happen',\n",
       " 'front',\n",
       " 'great',\n",
       " 'foot',\n",
       " 'run',\n",
       " 'mrs',\n",
       " 'hagrid',\n",
       " 'kreacher',\n",
       " 'goblin',\n",
       " 'hair',\n",
       " 'use',\n",
       " 'man',\n",
       " 'raise',\n",
       " 'let',\n",
       " 'while',\n",
       " 'under',\n",
       " 'another',\n",
       " 'muggle',\n",
       " 'potter',\n",
       " 'small',\n",
       " 'really',\n",
       " 'side',\n",
       " 'arm',\n",
       " 'few',\n",
       " 'mean',\n",
       " 'against',\n",
       " 'break',\n",
       " 'very',\n",
       " 'watch',\n",
       " 'floor',\n",
       " 'mr',\n",
       " 'people',\n",
       " 'day',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'year',\n",
       " 'call',\n",
       " 'speak',\n",
       " 'ministry',\n",
       " 'word',\n",
       " 'tent',\n",
       " 'help',\n",
       " 'whisper',\n",
       " 'walk',\n",
       " 'most',\n",
       " 'put',\n",
       " 'scream',\n",
       " 'second',\n",
       " 'black',\n",
       " 'family',\n",
       " 'locket',\n",
       " 'shout',\n",
       " 'both',\n",
       " 'kill',\n",
       " 'magic',\n",
       " 'beside',\n",
       " 'remember',\n",
       " 'half',\n",
       " 'shake',\n",
       " 'bill',\n",
       " 'begin',\n",
       " 'hogwart',\n",
       " 'three',\n",
       " 'believe',\n",
       " 'onto',\n",
       " 'our',\n",
       " 'glance',\n",
       " 'between',\n",
       " 'fly',\n",
       " 'those',\n",
       " 'griphook',\n",
       " 'ginny',\n",
       " 'start',\n",
       " 'wear',\n",
       " 'horcrux',\n",
       " 'stop',\n",
       " 'high',\n",
       " 'work',\n",
       " 'across',\n",
       " 'albus',\n",
       " 'should',\n",
       " 'without',\n",
       " 'air',\n",
       " 'each',\n",
       " 'scar',\n",
       " '\\\\\"but',\n",
       " 'night',\n",
       " 'realize',\n",
       " 'wall',\n",
       " 'hide',\n",
       " 'throw',\n",
       " 'along',\n",
       " 'talk',\n",
       " '\\\\\"and',\n",
       " 'luna',\n",
       " 'many',\n",
       " 'live',\n",
       " 'already',\n",
       " 'pass',\n",
       " 'catch',\n",
       " 'name',\n",
       " 'sound',\n",
       " 'however',\n",
       " 'suppose',\n",
       " 'appear',\n",
       " 'quite',\n",
       " 'beneath',\n",
       " 'finger',\n",
       " 'end',\n",
       " 'die',\n",
       " 'force',\n",
       " 'window',\n",
       " 'order',\n",
       " 'remain',\n",
       " 'same',\n",
       " 'step',\n",
       " 'anything',\n",
       " 'whether',\n",
       " 'fred',\n",
       " 'read',\n",
       " 'sight',\n",
       " 'cold',\n",
       " 'darkness',\n",
       " 'reach',\n",
       " 'wait',\n",
       " 'these',\n",
       " 'fleur',\n",
       " 'snake',\n",
       " 'everything',\n",
       " 'full',\n",
       " 'mother',\n",
       " 'bear',\n",
       " 'lead',\n",
       " 'mind',\n",
       " '\\\\\"you',\n",
       " 'set',\n",
       " 'yet',\n",
       " 'cry',\n",
       " 'boy',\n",
       " 'ground',\n",
       " 'table',\n",
       " 'life',\n",
       " 'top',\n",
       " 'drop',\n",
       " 'return',\n",
       " 'several',\n",
       " 'spell',\n",
       " 'white',\n",
       " 'bathilda',\n",
       " 'elf',\n",
       " 'new',\n",
       " 'expect',\n",
       " 'draw',\n",
       " 'else',\n",
       " 'far',\n",
       " 'hope',\n",
       " 'magical',\n",
       " 'vernon',\n",
       " 'book',\n",
       " 'forward',\n",
       " 'witch',\n",
       " 'together',\n",
       " 'large',\n",
       " 'deep',\n",
       " 'george',\n",
       " '\\\\\"he',\n",
       " 'follow',\n",
       " 'lose',\n",
       " 'silence',\n",
       " 'laugh',\n",
       " 'low',\n",
       " 'bag',\n",
       " 'able',\n",
       " 'uncle',\n",
       " 'silver',\n",
       " 'charm',\n",
       " 'bit',\n",
       " 'hard',\n",
       " 'stare',\n",
       " 'wonder',\n",
       " 'become',\n",
       " 'lift',\n",
       " 'continue',\n",
       " 'sirius',\n",
       " 'stone',\n",
       " 'golden',\n",
       " 'slightly',\n",
       " 'umbridge',\n",
       " 'next',\n",
       " 'plan',\n",
       " 'tonk',\n",
       " 'until',\n",
       " 'godric',\n",
       " 'change',\n",
       " 'invisibility',\n",
       " 'tiny',\n",
       " 'bellatrix',\n",
       " 'course',\n",
       " 'lord',\n",
       " 'pain',\n",
       " 'shoulder',\n",
       " 'young',\n",
       " 'cattermole',\n",
       " 'mad',\n",
       " 'rather',\n",
       " 'fear',\n",
       " 'kitchen',\n",
       " 'parent',\n",
       " 'curse',\n",
       " 'understand',\n",
       " 'bad',\n",
       " 'blood',\n",
       " 'rise',\n",
       " 'smile',\n",
       " 'always',\n",
       " 'question',\n",
       " 'real',\n",
       " 'vanish',\n",
       " 'dead',\n",
       " 'dudley',\n",
       " 'perhaps',\n",
       " 'near',\n",
       " 'professor',\n",
       " 'yell',\n",
       " 'figure',\n",
       " '\\\\\"it',\n",
       " 'miss',\n",
       " 'body',\n",
       " 'cast',\n",
       " 'glass',\n",
       " 'friend',\n",
       " 'tree',\n",
       " 'anyone',\n",
       " 'answer',\n",
       " 'idea',\n",
       " 'hall',\n",
       " 'heart',\n",
       " 'show',\n",
       " 'master',\n",
       " 'bed',\n",
       " 'regulus',\n",
       " 'send',\n",
       " 'stay',\n",
       " 'father',\n",
       " 'greyback',\n",
       " 'gryffindor',\n",
       " 'object',\n",
       " 'robe',\n",
       " '\\\\\"we',\n",
       " 'bring',\n",
       " 'whose',\n",
       " 'add',\n",
       " 'corner',\n",
       " 'hour',\n",
       " 'red',\n",
       " 'sense',\n",
       " 'steal',\n",
       " 'whole',\n",
       " 'attempt',\n",
       " 'hit',\n",
       " 'burn',\n",
       " 'hurry',\n",
       " 'mundungus',\n",
       " 'repeat',\n",
       " 'malfoy',\n",
       " 'nearly',\n",
       " 'tear',\n",
       " 'sorry',\n",
       " 'hollow',\n",
       " 'later',\n",
       " 'mouth',\n",
       " 'gaze',\n",
       " 'wizarde',\n",
       " 'such',\n",
       " 'ear',\n",
       " 'rest',\n",
       " 'clear',\n",
       " 'cover',\n",
       " 'empty',\n",
       " 'green',\n",
       " 'fire',\n",
       " 'meet',\n",
       " 'burst',\n",
       " 'chair',\n",
       " 'home',\n",
       " 'kingsley',\n",
       " 'picture',\n",
       " 'push',\n",
       " 'since',\n",
       " 'spot',\n",
       " 'alone',\n",
       " 'dementor',\n",
       " 'past',\n",
       " 'school',\n",
       " 'minute',\n",
       " 'number',\n",
       " 'outside',\n",
       " 'wave',\n",
       " 'destroy',\n",
       " 'join',\n",
       " 'fight',\n",
       " 'office',\n",
       " 'pick',\n",
       " 'pocket',\n",
       " 'enough',\n",
       " 'recognize',\n",
       " 'breath',\n",
       " 'jump',\n",
       " 'roar',\n",
       " 'week',\n",
       " 'fact',\n",
       " 'muriel',\n",
       " 'slip',\n",
       " 'notice',\n",
       " 'sign',\n",
       " 'forget',\n",
       " 'part',\n",
       " 'echo',\n",
       " 'brother',\n",
       " 'entrance',\n",
       " 'neck',\n",
       " 'themselves',\n",
       " 'clutch',\n",
       " 'gregorovitch',\n",
       " 'short',\n",
       " 'barely',\n",
       " 'chest',\n",
       " 'explain',\n",
       " 'hallow',\n",
       " 'horcruxe',\n",
       " 'seize',\n",
       " 'fast',\n",
       " 'manage',\n",
       " 'nobody',\n",
       " 'yaxley',\n",
       " 'child',\n",
       " 'nigellus',\n",
       " 'ought',\n",
       " 'also',\n",
       " 'water',\n",
       " '\\\\\"that',\n",
       " '\\\\\"what',\n",
       " 'case',\n",
       " 'reckon',\n",
       " 'slowly',\n",
       " 'thought',\n",
       " 'world',\n",
       " 'mention',\n",
       " 'phineas',\n",
       " 'imagine',\n",
       " 'mutter',\n",
       " 'ted',\n",
       " 'examine',\n",
       " 'four',\n",
       " 'finish',\n",
       " 'please',\n",
       " 'stuff',\n",
       " 'moody',\n",
       " 'gold',\n",
       " 'listen',\n",
       " 'page',\n",
       " 'scrimgeour',\n",
       " 'dursley',\n",
       " 'less',\n",
       " 'struggle',\n",
       " 'woman',\n",
       " 'james',\n",
       " 'safe',\n",
       " 'secret',\n",
       " 'skeeter',\n",
       " 'sleep',\n",
       " 'snow',\n",
       " 'ariana',\n",
       " 'decide',\n",
       " 'enter',\n",
       " 'letter',\n",
       " 'search',\n",
       " 'visit',\n",
       " 'corridor',\n",
       " 'dragon',\n",
       " 'mcgonagall',\n",
       " 'nod',\n",
       " 'probably',\n",
       " 'stair',\n",
       " 'blue',\n",
       " 'either',\n",
       " 'girl',\n",
       " 'hang',\n",
       " 'mark',\n",
       " 'nose',\n",
       " 'wrong',\n",
       " 'sister',\n",
       " 'least',\n",
       " 'wish',\n",
       " 'free',\n",
       " 'lot',\n",
       " 'neville',\n",
       " 'ceiling',\n",
       " 'chance',\n",
       " 'pant',\n",
       " 'petunia',\n",
       " 'shadow',\n",
       " 'xenophilius',\n",
       " 'matter',\n",
       " 'story',\n",
       " 'fill',\n",
       " 'grindelwald',\n",
       " 'it,\\\\',\n",
       " 'reply',\n",
       " 'straight',\n",
       " 'ago',\n",
       " 'drag',\n",
       " 'finally',\n",
       " 'photograph',\n",
       " 'shall',\n",
       " 'son',\n",
       " 'wide',\n",
       " 'bright',\n",
       " 'silent',\n",
       " 'somebody',\n",
       " 'you,\\\\',\n",
       " 'approach',\n",
       " 'edge',\n",
       " 'kind',\n",
       " '\\\\',\n",
       " 'aside',\n",
       " 'carry',\n",
       " 'grow',\n",
       " 'shoot',\n",
       " 'simply',\n",
       " 'thank',\n",
       " 'truth',\n",
       " 'shut',\n",
       " 'sort',\n",
       " 'above',\n",
       " 'agree',\n",
       " 'avoid',\n",
       " 'beyond',\n",
       " 'exactly',\n",
       " 'forest',\n",
       " 'press',\n",
       " 'save',\n",
       " 'someone',\n",
       " 'within',\n",
       " 'backward',\n",
       " 'disapparate',\n",
       " 'doge',\n",
       " 'it?\\\\',\n",
       " 'potion',\n",
       " 'soon',\n",
       " 'surround',\n",
       " 'wedding',\n",
       " 'arrive',\n",
       " 'herself',\n",
       " 'lily',\n",
       " 'aunt',\n",
       " 'bedroom',\n",
       " 'hardly',\n",
       " 'stick',\n",
       " 'apparently',\n",
       " 'cross',\n",
       " 'forehead',\n",
       " 'loud',\n",
       " 'mum',\n",
       " 'slide',\n",
       " 'bellow',\n",
       " 'completely',\n",
       " 'escape',\n",
       " 'except',\n",
       " 'ollivander',\n",
       " 'true',\n",
       " 'act',\n",
       " 'draco',\n",
       " 'everyone',\n",
       " 'itself',\n",
       " 'mirror',\n",
       " 'power',\n",
       " 'somewhere',\n",
       " 'thief',\n",
       " 'touch',\n",
       " '\\\\\"she',\n",
       " 'care',\n",
       " 'different',\n",
       " 'heavy',\n",
       " 'protection',\n",
       " 'cut',\n",
       " 'dare',\n",
       " 'dean',\n",
       " 'shock',\n",
       " 'stride',\n",
       " 'wake',\n",
       " 'attack',\n",
       " 'odd',\n",
       " 'paper',\n",
       " 'whom',\n",
       " 'yes',\n",
       " 'couple',\n",
       " 'noise',\n",
       " 'pack',\n",
       " 'seek',\n",
       " 'sky',\n",
       " 'chain',\n",
       " 'dirk',\n",
       " 'middle',\n",
       " 'morning',\n",
       " 'pale',\n",
       " 'pile',\n",
       " 'quickly',\n",
       " 'castle',\n",
       " 'cottage',\n",
       " 'gate',\n",
       " 'pause',\n",
       " 'pretend',\n",
       " 'roll',\n",
       " 'actually',\n",
       " 'beaded',\n",
       " 'crack',\n",
       " 'leap',\n",
       " 'pool',\n",
       " '\\\\\"harry',\n",
       " 'protect',\n",
       " 'crash',\n",
       " 'lucius',\n",
       " 'murmur',\n",
       " 'wood',\n",
       " 'dad',\n",
       " 'garden',\n",
       " 'love',\n",
       " 'possible',\n",
       " 'tremble',\n",
       " 'angry',\n",
       " 'check',\n",
       " 'eld',\n",
       " 'important',\n",
       " 'prophet',\n",
       " 'unable',\n",
       " 'grave',\n",
       " 'throat',\n",
       " 'track',\n",
       " 'gray',\n",
       " 'happy',\n",
       " 'knee',\n",
       " 'leg',\n",
       " 'maybe',\n",
       " 'rucksack',\n",
       " 'soul',\n",
       " 'sudden',\n",
       " 'suggest',\n",
       " 'travel',\n",
       " 'climb',\n",
       " 'movement',\n",
       " 'patronus',\n",
       " 'remove',\n",
       " '\\\\\"there',\n",
       " 'choose',\n",
       " 'clearly',\n",
       " 'doubt',\n",
       " 'expression',\n",
       " 'grip',\n",
       " 'ready',\n",
       " 'reveal',\n",
       " 'thick',\n",
       " 'bang',\n",
       " 'lean',\n",
       " 'rain',\n",
       " 'stretch',\n",
       " 'traver',\n",
       " 'wife',\n",
       " 'beat',\n",
       " 'feeling',\n",
       " 'instead',\n",
       " 'ravenclaw',\n",
       " 'right,\\\\',\n",
       " 'sink',\n",
       " 'apart',\n",
       " 'burrow',\n",
       " 'form',\n",
       " 'krum',\n",
       " 'rita',\n",
       " 'seat',\n",
       " 'student',\n",
       " 'tight',\n",
       " 'bottom',\n",
       " \"d'you\",\n",
       " 'easy',\n",
       " 'snitch',\n",
       " 'spend',\n",
       " '\\\\\"\\\\\"i',\n",
       " 'fireplace',\n",
       " 'kid',\n",
       " 'person',\n",
       " 'shriek',\n",
       " 'square',\n",
       " 'big',\n",
       " 'blow',\n",
       " 'thin',\n",
       " 'trace',\n",
       " 'whatever',\n",
       " 'color',\n",
       " 'dust',\n",
       " 'pair',\n",
       " 'ring',\n",
       " 'swing',\n",
       " 'cup',\n",
       " 'early',\n",
       " 'grimmauld',\n",
       " 'aberforth',\n",
       " 'drive',\n",
       " 'flame',\n",
       " 'human',\n",
       " '\\\\\"if',\n",
       " 'hug',\n",
       " 'kendra',\n",
       " 'pretty',\n",
       " 'vault',\n",
       " 'below',\n",
       " 'control',\n",
       " 'deathly',\n",
       " 'direction',\n",
       " 'fake',\n",
       " 'familiar',\n",
       " 'food',\n",
       " 'portrait',\n",
       " 'quietly',\n",
       " 'strange',\n",
       " 'auntie',\n",
       " 'bind',\n",
       " 'cause',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'delacour',\n",
       " 'dress',\n",
       " 'emerge',\n",
       " 'here,\\\\',\n",
       " 'hot',\n",
       " 'usual',\n",
       " 'yard',\n",
       " 'drink',\n",
       " 'enormous',\n",
       " 'glare',\n",
       " 'job',\n",
       " 'learn',\n",
       " 'neither',\n",
       " 'note',\n",
       " 'oh',\n",
       " 'effort',\n",
       " 'glide',\n",
       " 'riddle',\n",
       " 'suddenly',\n",
       " 'ten',\n",
       " 'trust',\n",
       " 'usually',\n",
       " 'write',\n",
       " 'broomstick',\n",
       " 'likely',\n",
       " 'me,\\\\',\n",
       " 'terrible',\n",
       " 'you?\\\\',\n",
       " '\\\\\"well',\n",
       " 'beam',\n",
       " 'bow',\n",
       " 'copy',\n",
       " 'gasp',\n",
       " 'group',\n",
       " 'haired',\n",
       " 'memory',\n",
       " 'phoenix',\n",
       " 'smell',\n",
       " 'sob',\n",
       " 'split',\n",
       " 'although',\n",
       " 'anyway',\n",
       " 'connection',\n",
       " 'deluminator',\n",
       " 'nor',\n",
       " 'summon',\n",
       " 'surface',\n",
       " 'freeze',\n",
       " 'information',\n",
       " 'minister',\n",
       " 'mudblood',\n",
       " 'severus',\n",
       " 'teach',\n",
       " 'tower',\n",
       " 'twelve',\n",
       " 'broken',\n",
       " 'enchantment',\n",
       " 'evening',\n",
       " 'land',\n",
       " 'line',\n",
       " 'refuse',\n",
       " 'sidecar',\n",
       " 'soar',\n",
       " 'tie',\n",
       " 'upward',\n",
       " 'anger',\n",
       " 'click',\n",
       " 'creature',\n",
       " 'er',\n",
       " 'five',\n",
       " 'flash',\n",
       " 'headmaster',\n",
       " 'invisible',\n",
       " 'news',\n",
       " 'owl',\n",
       " 'peer',\n",
       " 'smash',\n",
       " 'tone',\n",
       " 'address',\n",
       " 'hat',\n",
       " 'newspaper',\n",
       " 'powerful',\n",
       " 'present',\n",
       " 'row',\n",
       " 'shield',\n",
       " 'shine',\n",
       " 'smoke',\n",
       " 'tightly',\n",
       " 'tooth',\n",
       " 'upstairs',\n",
       " 'village',\n",
       " 'diadem',\n",
       " 'hole',\n",
       " 'protective',\n",
       " 'tall',\n",
       " 'torture',\n",
       " 'belong',\n",
       " 'canvas',\n",
       " 'crabbe',\n",
       " 'dedalus',\n",
       " 'desk',\n",
       " 'fit',\n",
       " 'gleam',\n",
       " 'gringotts',\n",
       " 'slam',\n",
       " 'accept',\n",
       " 'almost',\n",
       " 'cage',\n",
       " 'member',\n",
       " 'purple',\n",
       " 'round',\n",
       " 'skin',\n",
       " 'support',\n",
       " 'today',\n",
       " 'alive',\n",
       " 'art',\n",
       " 'broom',\n",
       " 'dance',\n",
       " 'discover',\n",
       " 'glad',\n",
       " 'slow',\n",
       " 'wind',\n",
       " 'yeah',\n",
       " 'age',\n",
       " 'christmas',\n",
       " 'harry,\\\\',\n",
       " 'impossible',\n",
       " 'lovegood',\n",
       " 'okay',\n",
       " 'rock',\n",
       " 'spin',\n",
       " 'strike',\n",
       " 'third',\n",
       " '\\\\\"i\\'m',\n",
       " 'bike',\n",
       " 'bunk',\n",
       " 'hedwig',\n",
       " 'motorbike',\n",
       " 'quick',\n",
       " 'rid',\n",
       " 'scene',\n",
       " 'single',\n",
       " 'hiding',\n",
       " 'inch',\n",
       " 'indeed',\n",
       " 'journey',\n",
       " 'know,\\\\',\n",
       " 'possess',\n",
       " 'road',\n",
       " 'street',\n",
       " 'trouble',\n",
       " 'amongst',\n",
       " 'fine',\n",
       " 'grab',\n",
       " 'lip',\n",
       " 'midair',\n",
       " 'panic',\n",
       " 'passage',\n",
       " 'surprised',\n",
       " 'symbol',\n",
       " 'thicknesse',\n",
       " 'clean',\n",
       " 'discuss',\n",
       " 'distant',\n",
       " 'goblet',\n",
       " 'halt',\n",
       " 'hesitate',\n",
       " 'history',\n",
       " 'merely',\n",
       " 'month',\n",
       " 'percy',\n",
       " 'pink',\n",
       " 'reason',\n",
       " 'scarlet',\n",
       " 'surprise',\n",
       " 'swallow',\n",
       " 'clothe',\n",
       " 'curl',\n",
       " 'curtain',\n",
       " 'downstairs',\n",
       " 'hestia',\n",
       " 'level',\n",
       " 'moan',\n",
       " 'surely',\n",
       " 'afraid',\n",
       " 'bald',\n",
       " 'breathe',\n",
       " 'carefully',\n",
       " 'crowd',\n",
       " 'daughter',\n",
       " 'difficult',\n",
       " 'glimpse',\n",
       " 'hundred',\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_itos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
