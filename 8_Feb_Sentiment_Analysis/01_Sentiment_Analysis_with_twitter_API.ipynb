{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with twitter API\n",
    "\n",
    "In this work, we want to find if the tag in twitter (e.g. \"iphone) is positive or negative by using\n",
    "sentiment analysis on the post related to tag. Then use our model to determine if it contains positive or negative sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Just loading and do some indexing. Nothing much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries.\n",
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "# Check if we can use CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Use for reproducability\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset for sentiment analysis.\n",
    "import pytreebank\n",
    "dataset = pytreebank.load_sst()\n",
    "example = dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n"
     ]
    }
   ],
   "source": [
    "# There are 5 labels for each sentence.\n",
    "# [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "# [0, 1, 2, 3, 4]\n",
    "for label, sentence in example.to_labeled_lines():\n",
    "    print(label)\n",
    "    print(sentence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(data):\n",
    "    temp_data  = list()\n",
    "    temp_label = list()\n",
    "    lenght = len(list(iter(data)))\n",
    "    for i in range(lenght):\n",
    "        for label, sentence in data[i].to_labeled_lines():\n",
    "            temp_data.append(sentence)\n",
    "            temp_label.append(label)\n",
    "    \n",
    "    return (temp_data, temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544 1101 2210\n",
      "318582 41447 82600\n",
      "318582 41447 82600\n"
     ]
    }
   ],
   "source": [
    "# We do not want to use the treebank structure.\n",
    "# Seperated into train, dev and test set\n",
    "train_data, train_label = unpack(dataset[\"train\"])\n",
    "valid_data, valid_label = unpack(dataset[\"dev\"])\n",
    "test_data,  test_label  = unpack(dataset[\"test\"])\n",
    "\n",
    "# Check the lenght of each data set\n",
    "train_size = len(train_data)\n",
    "valid_size = len(valid_data)\n",
    "test_size =  len(test_data)\n",
    "\n",
    "# This is the same as train_data above, it is just in the format of tree.\n",
    "# I do this for the sake of convenience when we trying to build dataloader.\n",
    "# But IT IS NOT A GOOD PRACTICE!\n",
    "train = dataset[\"train\"]\n",
    "valid = dataset[\"dev\"]\n",
    "test  = dataset[\"test\"]\n",
    "\n",
    "t_train_size = len(list(iter(train)))\n",
    "t_valid_size = len(list(iter(valid)))\n",
    "t_test_size = len(list(iter(test)))\n",
    "\n",
    "print(t_train_size, t_valid_size, t_test_size)\n",
    "print(train_size, valid_size, test_size)\n",
    "print(len(train_label), len(valid_label), len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n",
       " 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look\n",
    "train_data[0], train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numericalization\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):  #data_iter = train, test, validation\n",
    "    for data in data_iter:  # Look for the tree\n",
    "        for _, text in data.to_labeled_lines(): # Get the data inside tree\n",
    "            yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
    "                                                                 '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's don't know the word\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 919, 36, 2733, 9, 28, 908, 3233, 10]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Check if our vocab is working.\n",
    "print(vocab(['Chaky', 'wants', 'his', 'student', 'to', 'be', 'number', '1', '.']))\n",
    "print(vocab(['<pad>','<bos>','<eos>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index2word index.\n",
    "id2word = vocab.get_itos()\n",
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17136"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lenght of vocab\n",
    "len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Fasttext embedding\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "# Create fast_vectors\n",
    "#fast_vectors = FastText(language='en')\n",
    "\n",
    "# This is the code for saving the vector that we download as pickle.\n",
    "\n",
    "# import pickle \n",
    "# object = fast_vectors\n",
    "# filehandler = open('/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/data/Fast_vector_en.pkl', 'wb') \n",
    "# pickle.dump(object, filehandler)\n",
    "\n",
    "# If we already have the Fast vector in your device, you can use that.\n",
    "# Otherwise you need to download for a long time.\n",
    "import pickle\n",
    "filehandler = open('/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/data/Fast_vector_en.pkl', 'rb') \n",
    "fast_vectors = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we get the vectors, it's time to create embedding.\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17136, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check the shape\n",
    "fast_embedding.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "#label_pipeline = lambda x: int(x) - 1  #1, 2, 3, 4 ---> 0, 1, 2, 3 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing text_pipeline\n",
    "text_pipeline(\"I love to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.empty(3, 4, 5)\n",
    "t.size()\n",
    "torch.Size([3, 4, 5])\n",
    "t.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #making each batch same length\n",
    "\n",
    "pad_ix = vocab['<pad>']\n",
    "\n",
    "#this function gonna be called by DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0)) #for padding, this keep the lenght of sequence.\n",
    "        \n",
    "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
    "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
    "        torch.tensor(length_list, dtype=torch.int64)  # The pad_seq functions automatically do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the data in the format of tuples .. e.g. (label, text)\n",
    "\n",
    "def merge(list1, list2):\n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list\n",
    "\n",
    "\n",
    "training_data   = merge(train_label, train_data)\n",
    "validation_data = merge(valid_label, valid_data)\n",
    "testing_data    = merge(test_label,  test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the one that we created.\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The one that we already have.\n",
    "# Exactly the same!\n",
    "train_label[0], train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_loader   = DataLoader(validation_data, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "test_loader  = DataLoader(testing_data, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape:  torch.Size([64])\n",
      "Text shape:  torch.Size([64, 27])\n"
     ]
    }
   ],
   "source": [
    "for label, text, length in train_loader:\n",
    "    break\n",
    "print(\"Label shape: \", label.shape) # (batch_size, )\n",
    "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "Basically in this part, we will just define LSTM neural network and function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab)\n",
    "hid_dim    = 256\n",
    "emb_dim    = 300         # Why 300, we do not know depend on you.\n",
    "output_dim = 5 # [0, 1, 2, 3, 4] # We have 5 class\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "model.apply(initialize_weights)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5140800\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "  2560\n",
      "     5\n",
      "______\n",
      "7863109\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time.\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(val_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 46s\n",
      "\tTrain Loss: 0.468 | Train Acc: 81.03%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 82.25%\n",
      "Epoch: 02 | Time: 1m 48s\n",
      "\tTrain Loss: 0.351 | Train Acc: 85.33%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 82.07%\n",
      "Epoch: 03 | Time: 1m 53s\n",
      "\tTrain Loss: 0.311 | Train Acc: 86.96%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 81.69%\n",
      "Epoch: 04 | Time: 1m 56s\n",
      "\tTrain Loss: 0.283 | Train Acc: 88.08%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 82.19%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs      = 8\n",
    "tolerance_counter = 0\n",
    "\n",
    "save_path = f'/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/weights/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    valid_loss, valid_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        tolerance_counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')   \n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Tolerance techniques, stop the model if it start to overfit.\n",
    "    if tolerance_counter >= 3:\n",
    "        break\n",
    "\n",
    "    tolerance_counter = tolerance_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_length):\n",
    "    with torch.no_grad():\n",
    "        output = model(text, text_length).squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_checking(test_list):\n",
    "    predict_list = list()\n",
    "    for sent in test_list:\n",
    "        text = torch.tensor(text_pipeline(sent)).to(device)\n",
    "        text_list = [x.item() for x in text]\n",
    "        text = text.reshape(1, -1)\n",
    "        text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
    "        predict_list.append(predict(text, text_length))\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2], device='cuda:0'), tensor([1], device='cuda:0'), tensor([2], device='cuda:0'), tensor([2], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "#[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "test_case = ['The movie should have been good', # Negative\n",
    "    'What is not to like about this product.', # Negative\n",
    "    \"The price is not so bad\", # Positive\n",
    "    'This software is not buggy'] # Positive\n",
    "\n",
    "print(sentence_checking(test_case))\n",
    "\n",
    "# Our model is not so smart, but I this is the best that I can do for now.\n",
    "# I think it is mostly because the datasets is not that diverse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are in construction\n",
    "I tried to train with other datasets with the same model but I have a problem which I suspect occuring due to the batch size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model Accruacy\n",
    "As we can see from the results above our model is not so smart, I think the problem lie in the \n",
    "data diversity. Let's try to improve our model accruacy by feeding it's with more data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We want to make it learn imdb dataset with the same weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's import imdb dataset.\n",
    "df = pd.read_csv('/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/data/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the null values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we should clean it's a little bit and encoding the sentiment to positive and negative\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \n",
    "    # Clear the html tag by using regular expression.\n",
    "    sentence = re.sub(\"<[^>]*>\", \"\", sentence)\n",
    "    \n",
    "    stopwords = list(STOP_WORDS)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
    "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original.\n",
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a wonderful little production the film technique unassuming- old time bbc fashion give comforting discomforting sense realism entire piece the actor extremely chosen- michael sheen get polari voice pat you truly seamless editing guide reference williams ' diary entry worth watching terrificly write perform piece a masterful production great master comedy life the realism come home little thing fantasy guard use traditional dream technique remain solid disappear it play knowledge sense particularly scene concern orton halliwell set particularly flat halliwell mural decorate surface terribly\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The preprocessing one, look much better.\n",
    "preprocessing(df['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning to entire dataframe\n",
    "for i, row in df.iterrows():\n",
    "    clean_text = preprocessing(row.review)\n",
    "    df.at[i, 'Clean_review'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check the sentiment values\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the sentiment\n",
    "for i, row in df.iterrows():\n",
    "    cur_sentiment = row.sentiment\n",
    "    if cur_sentiment == 'positive':\n",
    "        df.at[i, 'Encoded_sentiment'] = 1\n",
    "    else:\n",
    "        df.at[i, 'Encoded_sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data cleaning csv for later use.\n",
    "#df.to_csv('/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/data/Cleaned_IMDB.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean_review</th>\n",
       "      <th>Encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mention watch 1 oz episode hook t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the film techniq...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think wonderful way spend time hot summer we...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Clean_review  Encoded_sentiment\n",
       "0  one reviewer mention watch 1 oz episode hook t...                1.0\n",
       "1  a wonderful little production the film techniq...                1.0\n",
       "2  i think wonderful way spend time hot summer we...                1.0\n",
       "3  basically family little boy jake think zombie ...                0.0\n",
       "4  petter mattei love time money visually stunnin...                1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to use the csv that we just save.\n",
    "import pandas as pd\n",
    "device = 'cpu' # We have some problems with the cuda so cpu will do for now.\n",
    "df = pd.read_csv('/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/data/Cleaned_IMDB.csv')\n",
    "df = df[[\"Clean_review\", \"Encoded_sentiment\"]] # Positive equal 1 and Negative equal 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 5000 10000\n"
     ]
    }
   ],
   "source": [
    "# Seperate data into train, valid and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train, valid = train_test_split(df, test_size=0.1)\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the df to the format that we want. e.g.. (label, text)\n",
    "def convert_format(df):\n",
    "    temp = list()\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[0]\n",
    "        label = row[1]\n",
    "        temp.append((label, text)) \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = convert_format(train)\n",
    "test  = convert_format(test)\n",
    "valid = convert_format(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spaz', 'definate', 'isenberg', 'age-', 'howie']\n",
      "113860\n"
     ]
    }
   ],
   "source": [
    "train_text = [text[1].split() for text in train]\n",
    "\n",
    "# Get word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(train_text))) # Unique set of word\n",
    "print(vocab[0:5])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "# Function to build the w2idx and id2word.\n",
    "def build_vocab(docs, max_vocab=1000000):\n",
    "\n",
    "  stoi = {'<unk>':0, '<pad>':1, '<bos>':2, '<eos>':3}\n",
    "  itos = {0:'<unk>', 1:'<pad>', 2:'<bos>', 3:'<eos>'}\n",
    "  idx = 4\n",
    "\n",
    "  for word in docs:\n",
    "    if word not in stoi:\n",
    "      if len(stoi) < max_vocab:\n",
    "        stoi[word] = idx\n",
    "        itos[idx] = word\n",
    "        idx += 1\n",
    "        \n",
    "  return stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to index and index to word with the function we just define.\n",
    "# It is in the dictionary form\n",
    "word2id, id2word = build_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline_2(sentence):\n",
    "    temp_list = list()\n",
    "    tokenizer_sentence = tokenizer(sentence)\n",
    "\n",
    "    for word in tokenizer_sentence:\n",
    "        try: \n",
    "            temp_list.append(word2id[word])\n",
    "        except:\n",
    "            temp_list.append(0)\n",
    "    return temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same one from the above\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #making each batch same length\n",
    "\n",
    "pad_ix = word2id['<pad>']\n",
    "\n",
    "#this function gonna be called by DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline_2(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0)) #for padding, this keep the lenght of sequence.\n",
    "        \n",
    "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
    "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
    "        torch.tensor(length_list, dtype=torch.int64)  # The pad_seq functions automatically do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_loader   = DataLoader(valid, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "test_loader  = DataLoader(test, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape:  torch.Size([32])\n",
      "Text shape:  torch.Size([32, 383])\n"
     ]
    }
   ],
   "source": [
    "# Just checking and import get_tokenizer here for the sake of convenience.\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "\n",
    "for label, text, length in train_loader:\n",
    "    break\n",
    "print(\"Label shape: \", label.shape) # (batch_size, )\n",
    "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# The first weight on the first layer and output is differents we need to filter first.\n",
    "# Before loading the model\n",
    "\n",
    "weight_path = '/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/weights/LSTM.pt'\n",
    "weight = torch.load(weight_path)\n",
    "print(len(weight))\n",
    "del weight['embedding.weight']\n",
    "del weight['fc.bias']\n",
    "del weight['fc.weight']\n",
    "print(len(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['embedding.weight', 'fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim  = len(vocab)\n",
    "hid_dim    = 256\n",
    "emb_dim    = 300  # Why 300, I do not know as it's mentioned above\n",
    "output_dim = 2 # [0, 1, 2, 3, 4] # Now we have 2 classes\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "model.load_state_dict(weight, strict=False)\n",
    "\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "for i, (label, text, text_length) in enumerate(train_loader):\n",
    "    print(label)\n",
    "    test_label = label\n",
    "    test_text = text   \n",
    "    test_text_lenght = text_length\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just put it inside CUDA and tried to make the model predict.\n",
    "test_text = test_text.to(device)\n",
    "test_text_lenght = test_text_lenght.to(device)\n",
    "test_output = model(test_text, test_text_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1024,  0.0394],\n",
       "        [ 0.0548,  0.1474],\n",
       "        [ 0.0612,  0.0926],\n",
       "        [ 0.1381,  0.0950],\n",
       "        [ 0.0538,  0.0623],\n",
       "        [ 0.0376,  0.0274],\n",
       "        [ 0.0808,  0.1127],\n",
       "        [ 0.0468,  0.1822],\n",
       "        [ 0.0519,  0.0928],\n",
       "        [ 0.0794,  0.0245],\n",
       "        [ 0.0770,  0.0867],\n",
       "        [ 0.0977,  0.0987],\n",
       "        [ 0.0656, -0.0213],\n",
       "        [ 0.0667,  0.0659],\n",
       "        [ 0.0707,  0.1548],\n",
       "        [ 0.0036,  0.1207],\n",
       "        [ 0.0814,  0.1427],\n",
       "        [ 0.0874,  0.1499],\n",
       "        [ 0.1174,  0.0801],\n",
       "        [ 0.0412,  0.0502],\n",
       "        [ 0.0514,  0.0947],\n",
       "        [ 0.1143,  0.0802],\n",
       "        [ 0.0899,  0.1118],\n",
       "        [-0.0225,  0.1646],\n",
       "        [ 0.0383, -0.0289],\n",
       "        [ 0.0582,  0.0272],\n",
       "        [ 0.0627,  0.1125],\n",
       "        [-0.0056,  0.0761],\n",
       "        [ 0.0535,  0.0509],\n",
       "        [ 0.1868,  0.0745],\n",
       "        [ 0.0803,  0.0533],\n",
       "        [ 0.0266, -0.0562]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's seem working\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "test_label = test_label.to(device)\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy\n",
    "\n",
    "loss = criterion(test_output, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length\n",
    "\n",
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length\n",
    "\n",
    "# Function to calculate time.\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(val_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mto(device) \u001b[39m#(batch_size, seq len)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#predict\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m predictions \u001b[39m=\u001b[39m model(text, text_length)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39m#calculate loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, label)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [13], line 18\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text, text_lengths):\n\u001b[1;32m     17\u001b[0m     \u001b[39m#text = [batch size, seq len]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(text)\n\u001b[1;32m     20\u001b[0m     \u001b[39m#++ pack sequence ++\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     packed_embedded \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(embedded, text_lengths\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs      = 8\n",
    "tolerance_counter = 0 # Initialize this counter\n",
    "save_path = f'/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/weights/LSTM2.pt'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    ### TRAINING ###\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(train_loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length).squeeze(1)\n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    train_loss, train_acc =  epoch_loss / train_loader_length, epoch_acc / train_loader_length\n",
    "\n",
    "    ### END OF TRAINING ###\n",
    "\n",
    "    ### START OF VALIDATION ###\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(val_loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    valid_loss, valid_acc =  epoch_loss / val_loader_length, epoch_acc / val_loader_length\n",
    "    \n",
    "\n",
    "    #train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    #valid_loss, valid_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        tolerance_counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')   \n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Tolerance techniques, stop the model if it start to overfit.\n",
    "    if tolerance_counter >= 3:\n",
    "        break\n",
    "\n",
    "    tolerance_counter = tolerance_counter + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
